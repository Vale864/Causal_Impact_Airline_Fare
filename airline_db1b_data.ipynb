{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMaH5P1lrc96AIOHXc7TZX9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"jXsUNLy-gGdW"}},{"cell_type":"code","source":["!pip install pyarrow fastparquet linearmodels -q"],"metadata":{"id":"WJAf40OjcX3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"‚úÖ Google Drive mounted successfully.\")\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import os\n","import glob\n","import zipfile\n","import logging\n","from tqdm.auto import tqdm\n","from typing import List, Dict Any, Tuple Optional\n","from collections import defaultdict\n","import requests\n","from io import BytesIO\n","\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","sns.set_style(\"whitegrid\")\n","\n","GOOGLE_DRIVE_BASE_DIR = '/content/drive/My Drive/AirlineData/'\n","RAW_DATA_DIR = os.path.join(GOOGLE_DRIVE_BASE_DIR, 'raw')\n","PROCESSED_DATA_DIR = os.path.join(GOOGLE_DRIVE_BASE_DIR, 'processed')\n","FINAL_DATAFRAMES_DIR = os.path.join(GOOGLE_DRIVE_BASE_DIR, 'processed_tables')\n","\n","os.makedirs(RAW_DATA_DIR, exist_ok=True)\n","os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n","os.makedirs(FINAL_DATAFRAMES_DIR, exist_ok=True)\n","\n","print(f\"‚úÖ Directory structure created in Google Drive:\")\n","print(f\"   - Raw data path: {RAW_DATA_DIR}\")\n","print(f\"   - Processed data path: {PROCESSED_DATA_DIR}\")\n","print(f\"   - Final dataframes path: {FINAL_DATAFRAMES_DIR}\")\n","print(\"‚úÖ Environment setup complete. All libraries are installed and imported.\")"],"metadata":{"id":"cJGpu3bfcTy4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fetch Data"],"metadata":{"id":"hGnynKHHgg58"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqVAIJXabvBj"},"outputs":[],"source":["START_YEAR = 2008\n","END_YEAR = 2016\n","STUDY_PERIOD = range(START_YEAR, END_YEAR + 1)\n","\n","\n","# Data will be filtered to include only these airports.\n","DELTA_HUBS = [\n","    'ATL',  # Atlanta Hartsfield-Jackson\n","    'DTW',  # Detroit Metropolitan\n","    'MSP',  # Minneapolis-Saint Paul\n","    'SLC',  # Salt Lake City\n","    'JFK',  # New York - JFK\n","    'LGA',  # New York - LaGuardia\n","    #'MEM',  # Memphis (De-hubbed September 3, 2013 )\n","]\n","AMERICAN_HUBS = [\n","    'DFW',  # Dallas/Fort Worth\n","    'ORD',  # Chicago O'Hare\n","    'MIA',  # Miami\n","    'LAX',  # Los Angeles International\n","    'JFK',  # New York - JFK\n","]\n","US_AIRLINES_HUBS = [\n","    'CLT',  # Charlotte Douglas\n","    'PHL',  # Philadelphia\n","    'DCA',  # Washington Reagan\n","    'PHX',  # Phoenix Sky Harbor\n","]\n","UNITED_HUBS = [\n","    'ORD',  # Chicago O'Hare\n","    'DEN',  # Denver\n","    'SFO',  # San Francisco\n","    'IAD',  # Washington Dulles\n","    'LAX',  # Los Angeles International\n","    'CLE',  # Cleveland (De-hubbed April 2014 - June 2014 )\n","]\n","CONTINENTAL_HUBS = [\n","    'EWR',  # Newark\n","    'IAH',  # Houston Intercontinental\n","    'CLE',  # Cleveland\n","]\n","LLC_BASES = [\n","    'ACY',  # Atlantic City\n","    'LAS',  # Las Vegas\n","    'BOS',  # Boston Logan\n","    'JFK',  # New York - JFK\n","    'IAD',  # Washington Dulles\n","    'ORD',  # Chicago O'Hare\n","    'DFW',  # Dallas/Fort Worth\n","    'DEN',  # Denver\n","    'FLL',  # Fort Lauderdale\n","    'MCO',  # Orlando International\n","]\n","TREATMENT_AIRPORTS = [\n","    'DTW',  # Detroit Metropolitan\n","    'FLL',  # Fort Lauderdale\n","    'MCO',  # Orlando International\n","    'TPA',  # Tampa\n","    'RSW',  # Fort Myers\n","]\n","NON_FLORIDA_LEISURE = [\n","    'ACY',  # Atlantic City\n","    'LAS',  # Las Vegas\n","    'PHX',  # Phoenix Sky Harbor\n","    'MSY',  # New Orleans\n","]\n","NON_LEISURE = [\n","    'LGA',  # New York - LaGuardia\n","    'DCA',  # Washington Reagan\n","    'MSP',  # Minneapolis-Saint Paul\n","    'ORD',  # Chicago O'Hare\n","    'ATL',  # Atlanta Hartsfield-Jackson\n","]\n","FLORIDA_LEISURE = [\n","    'PBI',  # West Palm Beach\n","    'EYW',  # Key West\n","    'SFB',  # Orlando Sanford\n","    'PIE',  # St. Pete-Clearwater\n","    'ECP',  # Panama City\n","]\n","FLORIDA_NON_LEISURE = [\n","    'MIA',  # Miami\n","    'JAX',  # Jacksonville\n","    'DAB',  # Daytona Beach\n","    'PNS',  # Pensacola\n","    'TLH',  # Tallahassee\n","    'MLB',  # Melbourne/Orlando\n","]\n","CARIBBEAN_BAHAMAS = [\n","    'CUN',  # Mexico\n","    'NAS',  # Bahamas\n","    'PUJ',  # Dominican Republic\n","    'MBJ',  # Puerto Rico\n","    'SJU',  # Dominican Republic\n","    'GCM',  # Cayman Islands\n","    'AUA',  # Aruba\n","]\n","LATIN_AMERICAN = [\n","    'BOG',  # Colombia\n","    'PTY',  # Panama\n","    'GRU',  # Brazil\n","    'LIM',  # Peru\n","    'MDE',  # Colombia\n","    'SJO',  # Costa Rica\n","]\n","\n","\n","# A combined list of all major legacy hubs for efficient filtering.\n","ALL_HUBS = [str(hub) for hub in np.unique(DELTA_HUBS\n","                                          + AMERICAN_HUBS\n","                                          + US_AIRLINES_HUBS\n","                                          + UNITED_HUBS\n","                                          + CONTINENTAL_HUBS\n","                                          + LLC_BASES\n","                                          + TREATMENT_AIRPORTS\n","                                          + NON_FLORIDA_LEISURE\n","                                          + NON_LEISURE\n","                                          + FLORIDA_LEISURE\n","                                          + FLORIDA_NON_LEISURE\n","                                          + CARIBBEAN_BAHAMAS\n","                                          + LATIN_AMERICAN\n",")]\n","\n","# Data will be filtered to include only these carriers.\n","LEGACY_CARRIERS = ['DL', 'AA', 'UA',] # Delta,  American, United\n","MERGED_CARRIERS = ['NW', 'US', 'CO', 'CS', 'FL'] # Northwest, US Airways, Continental, Continental Micronesia, AirTran\n","LCC_CARRIERS = ['WN', 'B6'] # Southwest, JetBlue\n","ULCC_CARRIERS = ['NK', 'G4', 'F9'] # Spirit, Allegiant, Frontier\n","\n","# Combined list of all carriers included in the study.\n","ALL_CARRIERS = LEGACY_CARRIERS + MERGED_CARRIERS + LCC_CARRIERS + ULCC_CARRIERS\n","\n","CHUNKSIZE = 100000\n","\n","print(\"‚úÖ Global constants and configuration loaded.\")\n","print(f\"   - Study Period: {START_YEAR} - {END_YEAR}\")\n","print(f\"   - Target Hubs for Filtering: {ALL_HUBS}\")\n","print(f\"   - Target Carriers for Filtering: {ALL_CARRIERS}\")\n","print(f\"   - Chunk Size for Processing: {CHUNKSIZE}\")\n","\n","COUPON_REQUIRED_COLS = [\n","    'ItinID', 'MktID', 'Year', 'Quarter', 'Origin', 'Dest', 'RPCarrier',\n","    'OpCarrier', 'SeqNum', 'Coupons', 'FareClass', 'Break',\n","    'CouponType', 'Distance',\n","]\n","\n","COUPON_OPTIMIZED_SCHEMA = {\n","    'ItinID': 'uint32',\n","    'MktID': 'uint32',\n","    'Year': 'uint16',\n","    'Quarter': 'uint8',\n","    'Origin': 'category',\n","    'Dest': 'category',\n","    'RPCarrier': 'category',\n","    'OpCarrier': 'category',\n","    'SeqNum': 'uint8',\n","    'Coupons': 'uint8',\n","    'FareClass': 'category',\n","    'Break': 'category',\n","    'CouponType': 'category',\n","    'Distance': 'uint16',\n","}\n","\n","MARKET_REQUIRED_COLS = [\n","    'ItinID', 'MktID', 'MktCoupons', 'Year', 'Quarter', 'Origin', 'OriginState',\n","    'Dest', 'DestState','RPCarrier', 'OpCarrier', 'Passengers',\n","    'MktFare', 'MktMilesFlown', 'OpCarrierChange',\n","]\n","\n","MARKET_OPTIMIZED_SCHEMA = {\n","    'ItinID': 'uint32',\n","    'MktID': 'uint32',\n","    'MktCoupons': 'uint8',\n","    'Year': 'uint16',\n","    'Quarter': 'uint8',\n","    'Origin': 'category',\n","    'OriginState': 'category',\n","    'Dest': 'category',\n","    'DestState': 'category',\n","    'RPCarrier': 'category',\n","    'OpCarrier': 'category',\n","    'Passengers': 'uint16',\n","    'MktFare': 'float32',\n","    'MktMilesFlown': 'uint16',\n","    'OpCarrierChange': 'uint8',\n","}\n","\n","TICKET_REQUIRED_COLS = [\n","    'ItinID', 'Coupons', 'Passengers', 'Year', 'Quarter',\n","    'Origin', 'RPCarrier', 'RoundTrip', 'FarePerMile', 'ItinFare', 'MilesFlown',\n","    'RoundTrip', 'OnLine',\n","\n","]\n","\n","\n","TICKET_OPTIMIZED_SCHEMA = {\n","    'ItinID': 'uint32',\n","    'Coupons': 'uint8',\n","    'Passengers': 'uint16',\n","    'Year': 'uint16',\n","    'Quarter': 'uint8',\n","    'Origin': 'category',\n","    'RPCarrier': 'category',\n","    'RoundTrip': 'uint8',\n","    'FarePerMile': 'float32',\n","    'ItinFare': 'float32',\n","    'MilesFlown': 'uint16',\n","    'RoundTrip': 'uint8',\n","    'OnLine': 'uint8',\n","}\n","\n","\n","print(\"‚úÖ Required columns and optimized schemas defined for Coupon, Market, and Ticket tables.\")\n","print(\"\\n--- Coupon Table Definitions ---\")\n","print(f\"   - COUPON_REQUIRED_COLS ({len(COUPON_REQUIRED_COLS)} columns): {COUPON_REQUIRED_COLS}\")\n","print(f\"   - COUPON_OPTIMIZED_SCHEMA ({len(COUPON_OPTIMIZED_SCHEMA)} columns): {COUPON_OPTIMIZED_SCHEMA}\")\n","\n","print(\"\\n--- Market Table Definitions ---\")\n","print(f\"   - MARKET_REQUIRED_COLS ({len(MARKET_REQUIRED_COLS)} columns): {MARKET_REQUIRED_COLS}\")\n","print(f\"   - MARKET_OPTIMIZED_SCHEMA ({len(MARKET_OPTIMIZED_SCHEMA)} columns): {MARKET_OPTIMIZED_SCHEMA}\")\n","\n","print(\"\\n--- Ticket Table Definitions ---\")\n","print(f\"   - TICKET_REQUIRED_COLS ({len(TICKET_REQUIRED_COLS)} columns): {TICKET_REQUIRED_COLS}\")\n","print(f\"   - TICKET_OPTIMIZED_SCHEMA ({len(TICKET_OPTIMIZED_SCHEMA)} columns): {TICKET_OPTIMIZED_SCHEMA}\")\n","\n","\n","\n","\n","# Helper Functions for Data Acquisition and Processing\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    handlers=[logging.StreamHandler()]\n",")\n","\n","def download_file(url: str, dest_path: str) -> bool:\n","    \"\"\"\n","    Downloads a single file from a given URL with a progress bar.\n","\n","    Args:\n","        url: The URL of the file to download.\n","        dest_path: The local path where the downloaded file will be saved.\n","\n","    Returns:\n","        True if the download was successful, False otherwise.\n","    \"\"\"\n","    filename = os.path.basename(dest_path)\n","    try:\n","        logging.info(f\"Attempting to download: {filename} from {url}\")\n","        with requests.get(url, stream=True, timeout=120) as r:\n","            r.raise_for_status()\n","            total_size = int(r.headers.get('content-length', 0))\n","            with open(dest_path, 'wb') as f, tqdm.wrapattr(\n","                f, \"write\", total=total_size, unit='B', unit_scale=True,\n","                desc=f\"Downloading {filename}\"\n","            ) as file_obj:\n","                for chunk in r.iter_content(chunk_size=8192):\n","                    file_obj.write(chunk)\n","        logging.info(f\"Successfully downloaded: {filename}\")\n","        return True\n","    except requests.exceptions.RequestException as e:\n","        logging.error(f\"Failed to download {filename}. Error: {e}\")\n","        return False\n","    except Exception as e:\n","        logging.error(f\"An unexpected error occurred during download of {filename}: {e}\")\n","        return False\n","\n","\n","def unzip_file(zip_path: str, extract_dir: str, expected_csv_name: str):\n","    \"\"\"\n","    Unzips a file, extracts a specific CSV, renames it to an expected name,\n","    and deletes the original zip archive.\n","\n","    Args:\n","        zip_path: Path to the input zip file.\n","        extract_dir: Directory where the CSV should be extracted.\n","        expected_csv_name: The desired name for the extracted CSV file.\n","    \"\"\"\n","    filename = os.path.basename(zip_path)\n","    logging.info(f\"Unzipping {filename}\")\n","    try:\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            csv_files_in_zip = [f for f in zip_ref.namelist() if f.lower().endswith('.csv')]\n","            if not csv_files_in_zip:\n","                logging.error(f\"No CSV file found in zip archive: {filename}\")\n","                return\n","            csv_to_extract = csv_files_in_zip[0]\n","            extracted_path = zip_ref.extract(csv_to_extract, extract_dir)\n","            final_csv_path = os.path.join(extract_dir, expected_csv_name)\n","            os.makedirs(os.path.dirname(final_csv_path), exist_ok=True)\n","            os.rename(extracted_path, final_csv_path)\n","            logging.info(f\"Successfully extracted and renamed '{csv_to_extract}' to '{expected_csv_name}' from: {filename}\")\n","        if os.path.exists(zip_path):\n","             os.remove(zip_path)\n","             logging.info(f\"Removed zip archive: {filename}\")\n","    except (zipfile.BadZipFile, FileNotFoundError, OSError) as e:\n","        logging.error(f\"Error unzipping or renaming file from {filename}: {e}\")\n","    except Exception as e:\n","        logging.error(f\"An unexpected error occurred during unzip of {filename}: {e}\")\n","\n","\n","def process_db1b_data(\n","    csv_path: str,\n","    output_path: str,\n","    required_cols: List[str],\n","    optimized_schema: Dict[str, str],\n","    table_name: str\n","):\n","    \"\"\"\n","    Reads a raw DB1B CSV file in chunks, applies cleaning, filtering, and\n","    transformation logic based on the table type (Coupon, Market, Ticket),\n","    and saves the processed data as an optimized Parquet file.\n","\n","    This function adapts the processing rules based on the input table:\n","    - Applies carrier filtering using 'RPCarrier' where available.\n","    - Applies data type optimization using the provided optimized_schema.\n","    - Includes specific filtering logic for the 'Ticket' table (ItinFare > 0, single coupon, hub origin).\n","    - Includes specific transformation logic for the 'Ticket' table (round-trip fare adjustment, passenger scaling).\n","\n","    Args:\n","        csv_path: Path to the raw input CSV file.\n","        output_path: Path to save the processed Parquet file.\n","        required_cols: List of columns to load from the CSV.\n","        optimized_schema: Dictionary mapping column names to optimized dtypes.\n","        table_name: The name of the DB1B table ('Coupon', 'Market', or 'Ticket').\n","    \"\"\"\n","    filename = os.path.basename(csv_path)\n","    logging.info(f\"Processing {filename} for {table_name} table...\")\n","\n","    try:\n","        csv_cols = pd.read_csv(csv_path, nrows=0).columns.str.strip()\n","        final_schema = {k: v for k, v in optimized_schema.items() if k in csv_cols}\n","        cols_to_use = [col for col in required_cols if col in csv_cols]\n","        missing_required_cols = set(required_cols) - set(cols_to_use)\n","        if missing_required_cols:\n","            logging.warning(f\"Required columns not found in {filename} for {table_name}: {missing_required_cols}. Processing with available columns.\")\n","\n","        critical_cols = ['ItinID']\n","        if table_name == 'Ticket':\n","             critical_cols.extend(['ItinFare', 'Passengers', 'Year', 'Quarter'])\n","        elif table_name == 'Market':\n","             critical_cols.extend(['MktFare', 'Passengers', 'Year', 'Quarter'])\n","        elif table_name == 'Coupon':\n","             critical_cols.extend(['Coupons'])\n","\n","\n","        if not all(col in cols_to_use for col in critical_cols):\n","             missing = set(critical_cols) - set(cols_to_use)\n","             logging.error(f\"Critical columns missing in {filename} for {table_name}: {missing}. Cannot process.\")\n","             return\n","\n","    except Exception as e:\n","        logging.error(f\"Could not read columns from {csv_path} for {table_name}. Error: {e}\")\n","        return\n","\n","    chunk_list = []\n","    try:\n","        with pd.read_csv(\n","            csv_path,\n","            usecols=cols_to_use,\n","            dtype={k: v for k, v in final_schema.items() if v != 'category'},\n","            chunksize=CHUNKSIZE,\n","            encoding='utf-8',\n","            low_memory=False\n","        ) as reader:\n","            for chunk in tqdm(reader, desc=f\"Filtering {filename}\"):\n","                if not all(col in chunk.columns for col in critical_cols):\n","                     logging.warning(f\"Skipping chunk in {filename} for {table_name} due to missing critical columns.\")\n","                     continue\n","\n","                processed_chunk = chunk.copy()\n","\n","                if 'RPCarrier' in processed_chunk.columns:\n","                     processed_chunk = processed_chunk[processed_chunk['RPCarrier'].isin(ALL_CARRIERS)].copy()\n","                #if 'OpCarrier' in processed_chunk.columns:\n","                     #processed_chunk = processed_chunk[processed_chunk['OpCarrier'].isin(ALL_CARRIERS)].copy()\n","\n","                #if 'Origin' in processed_chunk.columns and 'Dest' in processed_chunk.columns:\n","                    #processed_chunk = processed_chunk[\n","                        #(processed_chunk['Origin'].isin(ALL_HUBS)) |\n","                        #(processed_chunk['Dest'].isin(ALL_HUBS))\n","                    #].copy()\n","                #elif 'Origin' in processed_chunk.columns: # If only Origin is available, filter by Origin\n","                     #processed_chunk = processed_chunk[processed_chunk['Origin'].isin(ALL_HUBS)].copy()\n","                #elif 'Dest' in processed_chunk.columns: # If only Dest is available, filter by Dest\n","                     #processed_chunk = processed_chunk[processed_chunk['Dest'].isin(ALL_HUBS)].copy()\n","\n","                #if table_name == 'Ticket':\n","                    # Apply Ticket-specific rules from the MASTER PROMPT:\n","                    # Filter out invalid fares (ItinFare > 0).\n","                    #processed_chunk = processed_chunk[processed_chunk['ItinFare'] > 0].copy()\n","                if not processed_chunk.empty:\n","                    chunk_list.append(processed_chunk)\n","    except Exception as e:\n","        logging.error(f\"Error processing chunks for {csv_path} for {table_name}. Error: {e}\")\n","        return\n","\n","    if not chunk_list:\n","        logging.warning(f\"No relevant data found in {filename} for {table_name} after filtering. No output file generated.\")\n","        return\n","\n","    logging.info(f\"Concatenating {len(chunk_list)} chunks for {filename} for {table_name}...\")\n","    df = pd.concat(chunk_list, ignore_index=True)\n","    for col, dtype in final_schema.items():\n","        if dtype == 'category' and col in df.columns:\n","            df[col] = df[col].astype('category')\n","\n","    logging.info(f\"Final DataFrame for {filename} for {table_name} has {len(df):,} rows. Optimizing memory.\")\n","    logging.info(f\"Memory usage before saving: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n","\n","    try:\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        df.to_parquet(output_path, engine='pyarrow', compression='snappy')\n","        logging.info(f\"Successfully saved processed data to: {output_path}\")\n","    except Exception as e:\n","        logging.error(f\"Failed to save Parquet file {output_path} for {table_name}. Error: {e}\")\n","\n","print(\"‚úÖ Adapted helper functions for data acquisition and processing defined for all three tables.\")\n","\n","\n","# Data Acquisition & Processing Pipeline Execution\n","\n","DB1B_URL_TEMPLATE = \"https://transtats.bts.gov/PREZIP/Origin_and_Destination_Survey_DB1B{table}_{year}_{quarter}.zip\"\n","\n","DB1B_TABLES = {\n","    'Coupon': {'required_cols': COUPON_REQUIRED_COLS, 'optimized_schema': COUPON_OPTIMIZED_SCHEMA},\n","    'Market': {'required_cols': MARKET_REQUIRED_COLS, 'optimized_schema': MARKET_OPTIMIZED_SCHEMA},\n","    'Ticket': {'required_cols': TICKET_REQUIRED_COLS, 'optimized_schema': TICKET_OPTIMIZED_SCHEMA}\n","}\n","\n","\n","print(\"üöÄ Starting data engineering pipeline for Coupon, Market, and Ticket tables...\")\n","print(f\"   Processing data for the study period {START_YEAR}-Q1 to {END_YEAR}-Q4.\")\n","\n","for table_name, config in DB1B_TABLES.items():\n","    required_cols = config['required_cols']\n","    optimized_schema = config['optimized_schema']\n","\n","    print(f\"\\n--- Processing {table_name} table ---\")\n","\n","    for year in STUDY_PERIOD:\n","        for quarter in range(1, 5):\n","            if year > END_YEAR:\n","                continue\n","\n","            raw_file_name = f'db1b_{table_name.lower()}_{year}Q{quarter}.csv'\n","            raw_file_path = os.path.join(RAW_DATA_DIR, raw_file_name)\n","\n","            zip_file_name = f'Origin_and_Destination_Survey_DB1B{table_name}_{year}_{quarter}.zip'\n","            zip_file_path = os.path.join(RAW_DATA_DIR, zip_file_name)\n","\n","            processed_file_name = f'processed_{table_name.lower()}_{year}Q{quarter}.parquet'\n","            processed_file_path = os.path.join(PROCESSED_DATA_DIR, processed_file_name)\n","\n","            if os.path.exists(processed_file_path):\n","                print(f\"   - Processed file already exists, skipping: {processed_file_name}\")\n","                continue\n","\n","            try:\n","                if not os.path.exists(raw_file_path):\n","                    print(f\"   - Raw CSV not found for {table_name} {year}Q{quarter}. Attempting download and unzip to Google Drive raw directory...\")\n","                    download_url = DB1B_URL_TEMPLATE.format(table=table_name, year=year, quarter=quarter)\n","\n","                    if download_file(download_url, zip_file_path):\n","                        if os.path.exists(zip_file_path):\n","                            unzip_file(zip_file_path, RAW_DATA_DIR, raw_file_name)\n","                        if not os.path.exists(raw_file_path):\n","                             print(f\"   - ERROR: Raw CSV '{raw_file_name}' not found in Google Drive raw directory after unzipping '{zip_file_name}'. Skipping processing.\")\n","                             if os.path.exists(zip_file_path):\n","                                 os.remove(zip_file_path)\n","                             continue\n","                    else:\n","                        print(f\"   - WARNING: Download failed for {table_name} {year}Q{quarter}. Skipping processing.\")\n","                        continue\n","                else:\n","                     print(f\"   - Raw CSV already exists in Google Drive raw directory: {raw_file_name}. Skipping download/unzip.\")\n","\n","\n","                if os.path.exists(raw_file_path):\n","                    process_db1b_data(raw_file_path, processed_file_path, required_cols, optimized_schema, table_name)\n","\n","            except Exception as e:\n","                print(f\"   - ERROR processing {table_name} {year}Q{quarter}: {e}\")\n","\n","print(\"\\n‚úÖ Data engineering pipeline complete for all three tables.\")\n","print(\"üîÑ Loading processed data from Google Drive and aggregating Ticket table...\")\n","\n","parquet_files = sorted(glob.glob(os.path.join(PROCESSED_DATA_DIR, '*.parquet')))\n","\n","coupon_df_list = []\n","market_df_list = []\n","ticket_df_list = []\n","\n","if parquet_files:\n","    print(f\"Found {len(parquet_files)} processed parquet files in {PROCESSED_DATA_DIR}.\")\n","    for f in parquet_files:\n","        try:\n","            filename = os.path.basename(f)\n","            parts = filename.replace('.parquet', '').split('_')\n","            if len(parts) >= 3:\n","                 file_year_q = parts[-1] =\n","                 if 'Q' in file_year_q:\n","                      file_year = int(file_year_q.split('Q')[0])\n","                      if START_YEAR <= file_year <= END_YEAR:\n","                            if f'processed_coupon' in f:\n","                                coupon_df_list.append(pd.read_parquet(f))\n","                            elif f'processed_market' in f:\n","                                market_df_list.append(pd.read_parquet(f))\n","                            elif f'processed_ticket' in f:\n","                                ticket_df_list.append(pd.read_parquet(f))\n","                      else:\n","                           print(f\"   - Skipping file outside study period: {filename}\")\n","                 else:\n","                      print(f\"   - Skipping file with unexpected naming format: {filename}\")\n","            else:\n","                print(f\"   - Skipping file with unexpected naming format: {filename}\")\n","\n","        except Exception as e:\n","             print(f\"   - Error processing file {f}: {e}\")\n","\n","\n","    coupon_df = pd.concat(coupon_df_list, ignore_index=True) if coupon_df_list else pd.DataFrame()\n","    market_df = pd.concat(market_df_list, ignore_index=True) if market_df_list else pd.DataFrame()\n","    ticket_df = pd.concat(ticket_df_list, ignore_index=True) if ticket_df_list else pd.DataFrame()\n","\n","\n","    print(f\"‚úÖ Successfully loaded and aggregated data for the study period {START_YEAR}-{END_YEAR}.\")\n","    print(f\"   - Coupon records: {len(coupon_df):,}\")\n","    print(f\"   - Market records: {len(market_df):,}\")\n","    print(f\"   - Ticket records: {len(ticket_df):,}\")\n","\n","    print(\"\\n--- Coupon Data Preview ---\")\n","    display(coupon_df.head())\n","    print(\"\\n--- Market Data Preview ---\")\n","    display(market_df.head())\n","    print(\"\\n--- Ticket Data Preview ---\")\n","    display(ticket_df.head())\n","\n","else:\n","    print(\"‚ùå No processed Parquet files found in Google Drive. Cannot proceed with analysis.\")\n","\n","print(f\"\\nüíæ Saving final aggregated dataframes to {FINAL_DATAFRAMES_DIR}...\")\n","\n","if 'coupon_df' in locals() and not coupon_df.empty:\n","    final_coupon_path = os.path.join(FINAL_DATAFRAMES_DIR, 'final_coupon_df.parquet')\n","    coupon_df.to_parquet(final_coupon_path, engine='pyarrow', compression='snappy')\n","    print(f\"‚úÖ final_coupon_df.parquet saved to {final_coupon_path}.\")\n","else:\n","    print(\"‚ö†Ô∏è coupon_df is empty or not defined, skipping save.\")\n","\n","if 'market_df' in locals() and not market_df.empty:\n","    final_market_path = os.path.join(FINAL_DATAFRAMES_DIR, 'final_market_df.parquet')\n","    market_df.to_parquet(final_market_path, engine='pyarrow', compression='snappy')\n","    print(f\"‚úÖ final_market_df.parquet saved to {final_market_path}.\")\n","else:\n","    print(\"‚ö†Ô∏è market_df is empty or not defined, skipping save.\")\n","\n","if 'ticket_df' in locals() and not ticket_df.empty:\n","    final_ticket_path = os.path.join(FINAL_DATAFRAMES_DIR, 'final_ticket_df.parquet')\n","    ticket_df.to_parquet(final_ticket_path, engine='pyarrow', compression='snappy')\n","    print(f\"‚úÖ final_ticket_df.parquet saved to {final_ticket_path}.\")\n","else:\n","    print(\"‚ö†Ô∏è ticket_df is empty or not defined, skipping save.\")\n","\n","print(\"‚úÖ Attempted to save final dataframes.\")"]},{"cell_type":"markdown","source":["## Helper Functions"],"metadata":{"id":"fvzd2LvbjJhQ"}},{"cell_type":"code","source":["def pre_filter_check(df):\n","    \"\"\"Prints pre-filter diagnostics.\"\"\"\n","    print(\"Shape of data before filter:\", df.shape)\n","    print(\"Number of unique ItinIDs before filter:\", df['ItinID'].nunique())\n","    return df\n","\n","def add_time_index(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Prepares a DB1B DataFrame for CausalPy.\"\"\"\n","    df_copy = df.copy()\n","\n","    df_copy['QuarterPeriod'] = pd.PeriodIndex.from_fields(\n","        year=df_copy['Year'],\n","        quarter=df_copy['Quarter'],\n","        freq='Q'\n","    )\n","\n","    min_year = df_copy['Year'].min()\n","    df_copy['QuarterID'] = (df_copy['Year'] - min_year) * 4 + df_copy['Quarter']\n","\n","    return df_copy\n","\n","def add_period_indicator(df: pd.DataFrame,\n","                         intervention_point: pd.Period,\n","                         begin_study: pd.Period,\n","                         end_study: pd.Period) -> pd.DataFrame:\n","    \"\"\"Adds indicators for the post-treatment and full study periods.\"\"\"\n","    df_copy = df.copy()\n","\n","    is_post_treatment = ((df_copy['QuarterPeriod'] >= intervention_point) &\n","                         (df_copy['QuarterPeriod'] <= end_study))\n","\n","    is_study_period = ((df_copy['QuarterPeriod'] >= begin_study) &\n","                       (df_copy['QuarterPeriod'] <= end_study))\n","\n","    df_copy['ind_post_treatment'] = is_post_treatment.astype(int)\n","    df_copy['ind_study_period'] = is_study_period.astype(int)\n","\n","    return df_copy\n","\n","def filter_db1b_markets(df: pd.DataFrame,\n","                        origin_list: list,\n","                        dest_list: list,\n","                        min_fare: float = 50.0,\n","                        max_fare: float = 2000.0,\n","                        min_pas: int = 1,\n","                        max_pas: int = 10,\n","                        num_coupons: list = [1, 2, 3, 4],\n","                        switch_carrier: int = 0,\n","                        study_period: int = 1,\n","                        min_miles: float = 0.0) -> pd.DataFrame:\n","    \"\"\"\n","    Filters a DB1B market DataFrame for common analysis criteria.\n","\n","    Args:\n","        df: The raw DB1B market DataFrame.\n","        origin_list: A list of airport codes for the origin.\n","        dest_list: A list of airport codes for the destination.\n","        min_fare: The minimum MktFare to include.\n","        num_coupons: The exact number of MktCoupons to include.\n","        switch_carrier: The number of OpCarrierChanges allowed.\n","                          Default is 0 (no carrier changes).\n","        min_miles: The minimum MktMilesFlown to include.\n","\n","    Returns:\n","        A new, filtered DataFrame.\n","    \"\"\"\n","    is_route_match = ((df['Origin'].isin(origin_list)) &\n","                     (df['Dest'].isin(dest_list)))\n","    is_not_codeshare = (df['OpCarrier'] == df['RPCarrier'])\n","    is_valid_coupon = (df['MktCoupons'].isin(num_coupons))\n","    is_valid_fare = (df['MktFare'] > min_fare) & (df['MktFare'] < max_fare)\n","    is_valid_passenger = ((df['Passengers'] >= min_pas) &\n","                         (df['Passengers'] <= max_pas))\n","    is_valid_distance = (df['MktMilesFlown'] > min_miles)\n","    is_in_study_period = (df['ind_study_period'] == study_period)\n","    is_single_carrier = (df['OpCarrierChange'] == switch_carrier)\n","\n","    all_filters = (\n","        is_route_match &\n","        is_not_codeshare &\n","        is_valid_coupon &\n","        is_valid_fare &\n","        is_valid_passenger &\n","        is_valid_distance &\n","        is_in_study_period &\n","        is_single_carrier\n","    )\n","    return df[all_filters].copy()\n","\n","def add_route(df):\n","    \"\"\"Creates a standardized 'Route' column (e.g., 'ATL-JFK').\"\"\"\n","    col_1 = np.where(\n","        df['Origin'] < df['Dest'],\n","        df['Origin'],\n","        df['Dest']\n","    )\n","    col_2 = np.where(\n","        df['Origin'] < df['Dest'],\n","        df['Dest'],\n","        df['Origin']\n","    )\n","    return df.assign(Route = col_1 + '-' + col_2)\n","\n","def add_yield(df):\n","    \"\"\"\n","    Calculates passenger-weighted yield (Fare per Mile).\n","\n","    Yield = (Total MktFare * Passengers) / (Total MktMilesFlown * Passengers)\n","\n","    Handles division by zero by setting yield to 0.0 if miles are 0.\n","    \"\"\"\n","    total_fare = df['MktFare']\n","    total_miles = df['MktMilesFlown']\n","\n","    yield_col = np.where(\n","        total_miles > 0,\n","        total_fare / total_miles,\n","        0.0\n","    )\n","    return df.assign(Yield=yield_col)\n","\n","def recode_merged_carriers(df: pd.DataFrame,\n","                           verbose: bool = True) -> pd.DataFrame:\n","    \"\"\"\n","    Recodes 'RPCarrier' values in DB1B data to account for the\n","    economic reality of airline mergers, where reporting carrier codes\n","    lag the actual merger finalization.\n","\n","    This function standardizes carrier codes to their post-merger parent\n","    company, based on the quarter the merger became effective.\n","\n","    Args:\n","        df: The DB1B market DataFrame. Must contain 'Year', 'Quarter',\n","            and 'RPCarrier' columns.\n","        verbose: If True, prints a detailed log of how many records\n","                 were recoded for each merger.\n","\n","    Returns:\n","        A new DataFrame with 'RPCarrier' recoded.\n","    \"\"\"\n","    MERGER_DATA: List[Tuple[str, str, int, int, str]] = [\n","        ('NW', 'DL', 2008, 4, 'Delta-Northwest'),\n","        ('CO', 'UA', 2010, 4, 'United-Continental'),\n","        ('FL', 'WN', 2011, 2, 'Southwest-AirTran'),\n","        #('US', 'AA', 2014, 1, 'American-US Airways'),\n","        ('CS', 'UA', 2011, 1, 'Continental Micronesia-United')\n","    ]\n","\n","    df_recoded = df.copy()\n","    df_recoded['YearQuarter_Numeric'] = (\n","        df_recoded['Year'] + (df_recoded['Quarter'] / 10.0)\n","    )\n","\n","    if not pd.api.types.is_categorical_dtype(df_recoded['RPCarrier']):\n","        df_recoded['RPCarrier'] = df_recoded['RPCarrier'].astype('category')\n","\n","    present_carriers = set(df_recoded['RPCarrier'].cat.categories)\n","\n","    if verbose:\n","        print(\"--- Starting Airline Merger Recoding ---\")\n","\n","    for old_code, new_code, year, quarter, name in MERGER_DATA:\n","\n","        if old_code not in present_carriers:\n","            if verbose:\n","                print(f\"Skipping {name}: Carrier '{old_code}' not found in data.\")\n","            continue\n","\n","        merger_date_numeric = year + (quarter / 10.0)\n","\n","        mask = (df_recoded['RPCarrier'] == old_code) & \\\n","               (df_recoded['YearQuarter_Numeric'] >= merger_date_numeric)\n","\n","        num_affected = mask.sum()\n","\n","        if num_affected > 0:\n","            df_recoded.loc[mask, 'RPCarrier'] = new_code\n","            if verbose:\n","                print(f\"Recoded {num_affected:,} records for {name} merger ({old_code} -> {new_code}) \"\n","                      f\"on or after {year} Q{quarter}.\")\n","        elif verbose:\n","             print(f\"No records found to recode for {name} ({old_code}) after {year} Q{quarter}.\")\n","\n","    df_recoded['RPCarrier'] = df_recoded['RPCarrier'].cat.remove_unused_categories()\n","\n","    if verbose:\n","        print(\"\\n--- Recoding Complete ---\")\n","        print(\"Value counts for RPCarrier after recoding:\")\n","        print(df_recoded['RPCarrier'].value_counts())\n","\n","    df_recoded = df_recoded.drop(columns=['YearQuarter_Numeric'])\n","\n","    return df_recoded\n","\n","def post_filter_check(df):\n","    \"\"\"Prints post-filter diagnostics.\"\"\"\n","    print(\"Shape of data after filter:\", df.shape)\n","    print(\"Number of unique ItinIDs after filter:\", df['ItinID'].nunique())\n","    return df\n"],"metadata":{"id":"CL9kAeFJev8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["intervention_point = pd.Period('2012Q2', freq='Q')\n","begin_study = pd.Period('2008Q4', freq='Q')\n","end_study = pd.Period('2018Q2', freq='Q')\n","\n","clean_market_df = (\n","    market_df\n","    .pipe(pre_filter_check)\n","    .pipe(add_time_index)\n","    .pipe(add_period_indicator,\n","          intervention_point = intervention_point,\n","          begin_study = begin_study,\n","          end_study = end_study)\n","    .pipe(filter_db1b_markets,\n","          origin_list = ALL_HUBS,\n","          dest_list = ALL_HUBS)\n","    .pipe(add_route)\n","    .pipe(add_yield)\n","    .pipe(recode_merged_carriers)\n","    .pipe(post_filter_check)\n",")\n","\n","os.makedirs(FINAL_DATAFRAMES_DIR, exist_ok=True)\n","\n","FINAL_DATAFRAMES_DIR = '/content/drive/My Drive/AirlineData/processed_tables'\n","clean_market_path = os.path.join(FINAL_DATAFRAMES_DIR, 'clean_market_df.parquet')\n","\n","try:\n","    clean_market_df.to_parquet(clean_market_path, engine='pyarrow', compression='snappy')\n","    print(f\"‚úÖ clean_market_df saved to: {clean_market_path}\")\n","except Exception as e:\n","    print(f\"‚ùå Error saving clean_market_df: {e}\")"],"metadata":{"id":"caA9iQhxev50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dtw_fll_df = clean_market_df.copy()\n","\n","dtw_fll_df['Yield'] = dtw_fll_df['Yield'].astype('float64')\n","\n","dtw_fll_df = test_df[(test_df['QuarterPeriod'] <= '2015Q2') &\n","                  (test_df['Route'] == 'DTW-FLL')\n","]\n","\n","dtw_fll_df = add_time_index(dtw_fll_df)\n","\n","try:\n","    dtw_fll_df.to_parquet(dtw_fll_path, engine='pyarrow', compression='snappy')\n","    print(f\"‚úÖ dtw_fll_path saved to: {clean_market_path}\")\n","except Exception as e:\n","    print(f\"‚ùå Error saving dtw_fll_path: {e}\")"],"metadata":{"id":"5ZvrQIeqev28"},"execution_count":null,"outputs":[]}]}